{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596c3fbd",
   "metadata": {},
   "source": [
    "# Réentraînement sur données vectorisées (ref + prod)\n",
    "Ce notebook réentraîne un modèle à partir de features déjà vectorisées :\n",
    "- `ref_data.csv` : données de référence vectorisées (SVD)\n",
    "- `prod_data_vectorized*.csv` : feedback prod vectorisé (même schéma que ref, + colonne `prediction`)\n",
    "\n",
    "`DataModeling.ipynb` reste le notebook d'entraînement initial (texte → TF-IDF → SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92b28468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pipeline steps: ['tfidf', 'svd', 'clf']\n",
      "SVD n_components: 200\n"
     ]
    }
   ],
   "source": [
    "# Imports et configuration\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "DATA_DIR = Path('../data')\n",
    "ARTIFACT_DIR = Path('../artifacts')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REF_PATH = DATA_DIR / 'ref_data.csv'\n",
    "PROD_PATH = DATA_DIR / 'prod_data.csv'\n",
    "\n",
    "# Écraser le modèle et les métriques utilisés en prod\n",
    "MODEL_OUT = ARTIFACT_DIR / 'phishing_tfidf_logreg.joblib'\n",
    "METRICS_OUT = ARTIFACT_DIR / 'metrics.json'\n",
    "\n",
    "# Charger les artefacts figés du modèle initial\n",
    "original_pipeline = joblib.load(ARTIFACT_DIR / 'phishing_tfidf_logreg.joblib')\n",
    "svd_frozen = joblib.load(ARTIFACT_DIR / 'svd_ref.joblib')\n",
    "\n",
    "print(f\"Original pipeline steps: {[name for name, _ in original_pipeline.steps]}\")\n",
    "print(f\"SVD n_components: {svd_frozen.n_components}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0be5b1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref shape (5000, 201) prod shape (8, 201)\n",
      "concat shape (5008, 201)\n"
     ]
    }
   ],
   "source": [
    "# Chargement ref et prod vectorisés\n",
    "ref_df = pd.read_csv(REF_PATH)\n",
    "prod_df = pd.read_csv(PROD_PATH)\n",
    "\n",
    "# Retirer la colonne prediction du fichier prod (elle ne sert pas à l'entraînement)\n",
    "if 'prediction' in prod_df.columns:\n",
    "    prod_df = prod_df.drop(columns=['prediction'])\n",
    "if 'proba_phishing' in prod_df.columns:\n",
    "    prod_df = prod_df.drop(columns=['proba_phishing'])\n",
    "\n",
    "# Vérification du schéma (les features doivent matcher)\n",
    "ref_features = [c for c in ref_df.columns if c != 'target']\n",
    "prod_features = [c for c in prod_df.columns if c != 'target']\n",
    "\n",
    "missing_in_prod = [c for c in ref_features if c not in prod_features]\n",
    "extra_in_prod = [c for c in prod_features if c not in ref_features]\n",
    "if missing_in_prod or extra_in_prod:\n",
    "    raise ValueError(f\"Schéma incohérent. Manquantes: {missing_in_prod[:5]} | Extra: {extra_in_prod[:5]}\")\n",
    "\n",
    "# Réordonner prod pour coller exactement à ref (par prudence)\n",
    "prod_df = prod_df[ref_features + ['target']]\n",
    "\n",
    "print('ref shape', ref_df.shape, 'prod shape', prod_df.shape)\n",
    "\n",
    "# Concat ref + prod\n",
    "train_df = pd.concat([ref_df, prod_df], axis=0, ignore_index=True)\n",
    "print('concat shape', train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce5072a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3505, 200) val (751, 200) test (752, 200)\n"
     ]
    }
   ],
   "source": [
    "# Features / cible\n",
    "feature_cols = [c for c in train_df.columns if c != 'target']\n",
    "X = train_df[feature_cols].values\n",
    "y = train_df['target'].astype(int).values\n",
    "\n",
    "# Split 70 / 15 / 15 (stratifié),\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.50, random_state=RANDOM_SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "print('train', X_train.shape, 'val', X_val.shape, 'test', X_test.shape)\n",
    "\n",
    "# Entraînement final sur train + val\n",
    "X_train_final = np.concatenate([X_train, X_val])\n",
    "y_train_final = np.concatenate([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "268d3696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modèle entraîné (train+val)\n"
     ]
    }
   ],
   "source": [
    "# Entraînement Logistic Regression sur features déjà vectorisées (train + val)\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "clf.fit(X_train_final, y_train_final)\n",
    "print('modèle entraîné (train+val)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6bccd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== val ==\n",
      "Accuracy: 0.9640 | Precision: 0.9223 | Recall: 0.9856 | F1: 0.9529\n",
      "AUC: 0.9964\n",
      "Confusion matrix:\n",
      " [[451  23]\n",
      " [  4 273]]\n",
      "== test ==\n",
      "Accuracy: 0.9681 | Precision: 0.9502 | Recall: 0.9639 | F1: 0.9570\n",
      "AUC: 0.9957\n",
      "Confusion matrix:\n",
      " [[461  14]\n",
      " [ 10 267]]\n"
     ]
    }
   ],
   "source": [
    "# Évaluation\n",
    "def evaluate(model, X_eval, y_eval, label='eval'):\n",
    "    y_pred = model.predict(X_eval)\n",
    "    y_score = model.predict_proba(X_eval)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    acc = accuracy_score(y_eval, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_eval, y_pred, average='binary', zero_division=0)\n",
    "    auc = roc_auc_score(y_eval, y_score) if y_score is not None else None\n",
    "    cm = confusion_matrix(y_eval, y_pred)\n",
    "\n",
    "    print(f\"== {label} ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {p:.4f} | Recall: {r:.4f} | F1: {f1:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "    print('Confusion matrix:\\n', cm)\n",
    "\n",
    "    out = {'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1}\n",
    "    if auc is not None:\n",
    "        out['auc'] = auc\n",
    "    return out\n",
    "\n",
    "metrics = {\n",
    "    'val': evaluate(clf, X_val, y_val, 'val'),\n",
    "    'test': evaluate(clf, X_test, y_test, 'test')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4ecfd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline réentraîné (TF-IDF + SVD + nouvelle LR) enregistré: ..\\artifacts\\phishing_tfidf_logreg.joblib\n",
      "Métriques enregistrées: ..\\artifacts\\metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde modèle + métriques\n",
    "# SOLUTION: Reconstruire le pipeline avec TF-IDF + SVD + nouvelle LR\n",
    "# Extraire TF-IDF du pipeline original et ajouter SVD + nouvelle LR\n",
    "tfidf_component = original_pipeline.named_steps['tfidf']\n",
    "\n",
    "retrained_pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_component),  # TF-IDF figé du modèle initial\n",
    "    ('svd', svd_frozen),          # SVD figé (200 dim)\n",
    "    ('clf', clf)                  # Nouvelle LogisticRegression réentraînée\n",
    "])\n",
    "\n",
    "joblib.dump(retrained_pipeline, MODEL_OUT)\n",
    "with open(METRICS_OUT, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Pipeline réentraîné (TF-IDF + SVD + nouvelle LR) enregistré:', MODEL_OUT)\n",
    "print('Métriques enregistrées:', METRICS_OUT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
