{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596c3fbd",
   "metadata": {},
   "source": [
    "# Réentraînement sur données vectorisées (ref + prod)\n",
    "Ce notebook réentraîne un modèle à partir de features déjà vectorisées :\n",
    "- `ref_data.csv` : données de référence vectorisées (SVD)\n",
    "- `prod_data_vectorized*.csv` : feedback prod vectorisé (même schéma que ref, + colonne `prediction`)\n",
    "\n",
    "`DataModeling.ipynb` reste le notebook d'entraînement initial (texte → TF-IDF → SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b28468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:04:10.444956Z",
     "iopub.status.busy": "2026-01-27T22:04:10.444845Z",
     "iopub.status.idle": "2026-01-27T22:04:13.039418Z",
     "shell.execute_reply": "2026-01-27T22:04:13.038481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_212/3610627981.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pipeline steps: ['tfidf', 'svd', 'clf']\n",
      "SVD n_components: 200\n"
     ]
    }
   ],
   "source": [
    "# Imports et configuration\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "DATA_DIR = Path('../data')\n",
    "ARTIFACT_DIR = Path('../artifacts')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REF_PATH = DATA_DIR / 'ref_data.csv'\n",
    "PROD_PATH = DATA_DIR / 'prod_data.csv'\n",
    "\n",
    "# Écraser le modèle et les métriques utilisés en prod\n",
    "MODEL_OUT = ARTIFACT_DIR / 'phishing_tfidf_logreg.joblib'\n",
    "METRICS_OUT = ARTIFACT_DIR / 'metrics.json'\n",
    "\n",
    "# Charger les artefacts figés du modèle initial\n",
    "original_pipeline = joblib.load(ARTIFACT_DIR / 'phishing_tfidf_logreg.joblib')\n",
    "svd_frozen = joblib.load(ARTIFACT_DIR / 'svd_ref.joblib')\n",
    "\n",
    "print(f\"Original pipeline steps: {[name for name, _ in original_pipeline.steps]}\")\n",
    "print(f\"SVD n_components: {svd_frozen.n_components}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be5b1ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:04:13.041475Z",
     "iopub.status.busy": "2026-01-27T22:04:13.041227Z",
     "iopub.status.idle": "2026-01-27T22:04:13.351946Z",
     "shell.execute_reply": "2026-01-27T22:04:13.351246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref shape (5000, 201) prod shape (15, 201)\n",
      "concat shape (5015, 201)\n"
     ]
    }
   ],
   "source": [
    "# Chargement ref et prod vectorisés\n",
    "ref_df = pd.read_csv(REF_PATH)\n",
    "prod_df = pd.read_csv(PROD_PATH)\n",
    "\n",
    "# Retirer la colonne prediction du fichier prod (elle ne sert pas à l'entraînement)\n",
    "if 'prediction' in prod_df.columns:\n",
    "    prod_df = prod_df.drop(columns=['prediction'])\n",
    "if 'proba_phishing' in prod_df.columns:\n",
    "    prod_df = prod_df.drop(columns=['proba_phishing'])\n",
    "\n",
    "# Vérification du schéma (les features doivent matcher)\n",
    "ref_features = [c for c in ref_df.columns if c != 'target']\n",
    "prod_features = [c for c in prod_df.columns if c != 'target']\n",
    "\n",
    "missing_in_prod = [c for c in ref_features if c not in prod_features]\n",
    "extra_in_prod = [c for c in prod_features if c not in ref_features]\n",
    "if missing_in_prod or extra_in_prod:\n",
    "    raise ValueError(f\"Schéma incohérent. Manquantes: {missing_in_prod[:5]} | Extra: {extra_in_prod[:5]}\")\n",
    "\n",
    "# Réordonner prod pour coller exactement à ref (par prudence)\n",
    "prod_df = prod_df[ref_features + ['target']]\n",
    "\n",
    "print('ref shape', ref_df.shape, 'prod shape', prod_df.shape)\n",
    "\n",
    "# Concat ref + prod\n",
    "train_df = pd.concat([ref_df, prod_df], axis=0, ignore_index=True)\n",
    "print('concat shape', train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5072a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:04:13.353606Z",
     "iopub.status.busy": "2026-01-27T22:04:13.353453Z",
     "iopub.status.idle": "2026-01-27T22:04:13.373536Z",
     "shell.execute_reply": "2026-01-27T22:04:13.372193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3510, 200) val (752, 200) test (753, 200)\n"
     ]
    }
   ],
   "source": [
    "# Features / cible\n",
    "feature_cols = [c for c in train_df.columns if c != 'target']\n",
    "X = train_df[feature_cols].values\n",
    "y = train_df['target'].astype(int).values\n",
    "\n",
    "# Split 70 / 15 / 15 (stratifié),\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.50, random_state=RANDOM_SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "print('train', X_train.shape, 'val', X_val.shape, 'test', X_test.shape)\n",
    "\n",
    "# Entraînement final sur train + val\n",
    "X_train_final = np.concatenate([X_train, X_val])\n",
    "y_train_final = np.concatenate([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268d3696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:04:13.375317Z",
     "iopub.status.busy": "2026-01-27T22:04:13.375179Z",
     "iopub.status.idle": "2026-01-27T22:04:13.825808Z",
     "shell.execute_reply": "2026-01-27T22:04:13.824672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modèle entraîné (train+val)\n"
     ]
    }
   ],
   "source": [
    "# Entraînement Logistic Regression sur features déjà vectorisées (train + val)\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "clf.fit(X_train_final, y_train_final)\n",
    "print('modèle entraîné (train+val)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6bccd90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:04:13.830899Z",
     "iopub.status.busy": "2026-01-27T22:04:13.829649Z",
     "iopub.status.idle": "2026-01-27T22:04:13.920663Z",
     "shell.execute_reply": "2026-01-27T22:04:13.918135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== val ==\n",
      "Accuracy: 0.9734 | Precision: 0.9479 | Recall: 0.9820 | F1: 0.9647\n",
      "AUC: 0.9976\n",
      "Confusion matrix:\n",
      " [[459  15]\n",
      " [  5 273]]\n",
      "== test ==\n",
      "Accuracy: 0.9562 | Precision: 0.9239 | Recall: 0.9604 | F1: 0.9418\n",
      "AUC: 0.9937\n",
      "Confusion matrix:\n",
      " [[453  22]\n",
      " [ 11 267]]\n"
     ]
    }
   ],
   "source": [
    "# Évaluation\n",
    "def evaluate(model, X_eval, y_eval, label='eval'):\n",
    "    y_pred = model.predict(X_eval)\n",
    "    y_score = model.predict_proba(X_eval)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    acc = accuracy_score(y_eval, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_eval, y_pred, average='binary', zero_division=0)\n",
    "    auc = roc_auc_score(y_eval, y_score) if y_score is not None else None\n",
    "    cm = confusion_matrix(y_eval, y_pred)\n",
    "\n",
    "    print(f\"== {label} ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {p:.4f} | Recall: {r:.4f} | F1: {f1:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "    print('Confusion matrix:\\n', cm)\n",
    "\n",
    "    out = {'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1}\n",
    "    if auc is not None:\n",
    "        out['auc'] = auc\n",
    "    return out\n",
    "\n",
    "metrics = {\n",
    "    'val': evaluate(clf, X_val, y_val, 'val'),\n",
    "    'test': evaluate(clf, X_test, y_test, 'test')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ecfd25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:04:13.925634Z",
     "iopub.status.busy": "2026-01-27T22:04:13.925290Z",
     "iopub.status.idle": "2026-01-27T22:04:14.870160Z",
     "shell.execute_reply": "2026-01-27T22:04:14.868507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline réentraîné (TF-IDF + SVD + nouvelle LR) enregistré: ../artifacts/phishing_tfidf_logreg.joblib\n",
      "Métriques enregistrées: ../artifacts/metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde modèle + métriques\n",
    "# SOLUTION: Reconstruire le pipeline avec TF-IDF + SVD + nouvelle LR\n",
    "# Extraire TF-IDF du pipeline original et ajouter SVD + nouvelle LR\n",
    "tfidf_component = original_pipeline.named_steps['tfidf']\n",
    "\n",
    "retrained_pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_component),  # TF-IDF figé du modèle initial\n",
    "    ('svd', svd_frozen),          # SVD figé (200 dim)\n",
    "    ('clf', clf)                  # Nouvelle LogisticRegression réentraînée\n",
    "])\n",
    "\n",
    "joblib.dump(retrained_pipeline, MODEL_OUT)\n",
    "with open(METRICS_OUT, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Pipeline réentraîné (TF-IDF + SVD + nouvelle LR) enregistré:', MODEL_OUT)\n",
    "print('Métriques enregistrées:', METRICS_OUT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
